#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
GPT-4o å›¾æ ‡è¯†åˆ«æ¼”ç¤ºè„šæœ¬
åŸºäº demo.ipynb æ”¹å†™ï¼Œä½¿ç”¨ GPT-4o æ›¿ä»£ Florence-2

åŠŸèƒ½æè¿°:
- ä½¿ç”¨ YOLO æ¨¡å‹è¿›è¡Œå›¾æ ‡æ£€æµ‹
- ä½¿ç”¨ GPT-4o è¿›è¡Œå›¾æ ‡æè¿°å’Œåˆ†æ
- ä½¿ç”¨ PaddleOCR è¿›è¡Œæ–‡æœ¬è¯†åˆ«
- ç”Ÿæˆæ ‡æ³¨å›¾åƒå’Œè¯¦ç»†ç»“æœæ•°æ®

ä½¿ç”¨æ–¹æ³•:
1. ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ:
   python examples/basic/demo_gpt4o.py

2. ä»å½“å‰ç›®å½•è¿è¡Œ:
   cd examples/basic && python demo_gpt4o.py

ä¾èµ–è¦æ±‚:
- å·²å®‰è£…æ‰€æœ‰ä¾èµ–: pip install -r requirements.txt
- é…ç½®æ–‡ä»¶: config.json (åŒ…å« OpenAI API å¯†é’¥)
- æ¨¡å‹æƒé‡: weights/icon_detect/model.pt
- æµ‹è¯•å›¾åƒ: imgs/word.png, imgs/windows_home.png, imgs/google_page.png

è¾“å‡ºæ–‡ä»¶:
- output_gpt4o_*.png: å¸¦æ ‡æ³¨çš„å›¾åƒæ–‡ä»¶
- results_gpt4o_*.csv: è¯¦ç»†çš„æ£€æµ‹ç»“æœæ•°æ®

ç¤ºä¾‹å‘½ä»¤:
# åŸºç¡€è¿è¡Œ
python examples/basic/demo_gpt4o.py

# æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯
python examples/basic/demo_gpt4o.py --help

æ³¨æ„äº‹é¡¹:
- ç¡®ä¿ config.json ä¸­é…ç½®äº†æœ‰æ•ˆçš„ OpenAI API å¯†é’¥
- é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹æƒé‡
- CPU æ¨¡å¼ä¸‹å¤„ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨ GPU
"""

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import argparse
import time
import base64
import io
from PIL import Image
import torch
import pandas as pd

# å¯¼å…¥ OmniParser ç›¸å…³æ¨¡å—
from src.utils.utils import get_som_labeled_img, check_ocr_box, get_caption_model_processor, get_yolo_model
from src.utils.config import get_config

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="GPT-4o å›¾æ ‡è¯†åˆ«æ¼”ç¤ºè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºç¡€è¿è¡Œ (å¤„ç†é»˜è®¤å›¾åƒ)
  python examples/basic/demo_gpt4o.py
  
  # å¤„ç†å•ä¸ªå›¾åƒ
  python examples/basic/demo_gpt4o.py --image imgs/word.png
  
  # å¤„ç†å¤šä¸ªå›¾åƒ
  python examples/basic/demo_gpt4o.py --image imgs/word.png imgs/windows_home.png
  
  # å¯ç”¨ GPU åŠ é€Ÿ
  python examples/basic/demo_gpt4o.py --device cuda
  
  # è°ƒæ•´æ£€æµ‹é˜ˆå€¼
  python examples/basic/demo_gpt4o.py --threshold 0.1
  
  # å¯ç”¨è¯¦ç»†è¾“å‡º
  python examples/basic/demo_gpt4o.py --verbose

è¾“å‡ºæ–‡ä»¶:
  - output_gpt4o_*.png: å¸¦æ ‡æ³¨çš„å›¾åƒæ–‡ä»¶
  - results_gpt4o_*.csv: è¯¦ç»†çš„æ£€æµ‹ç»“æœæ•°æ®
  
æ³¨æ„äº‹é¡¹:
  - ç¡®ä¿ config.json ä¸­é…ç½®äº†æœ‰æ•ˆçš„ OpenAI API å¯†é’¥
  - ç¡®ä¿æ¨¡å‹æƒé‡æ–‡ä»¶å­˜åœ¨: weights/icon_detect/model.pt
  - CPU æ¨¡å¼ä¸‹å¤„ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨ GPU
        """
    )
    
    parser.add_argument(
        '--image', '-i',
        type=str,
        nargs='*',
        help='è¦å¤„ç†çš„å›¾åƒæ–‡ä»¶è·¯å¾„ (å¯æŒ‡å®šå¤šä¸ª)'
    )
    
    parser.add_argument(
        '--device', '-d',
        type=str,
        choices=['auto', 'cpu', 'cuda'],
        default='auto',
        help='æŒ‡å®šè®¡ç®—è®¾å¤‡ (é»˜è®¤: auto)'
    )
    
    parser.add_argument(
        '--threshold', '-t',
        type=float,
        default=0.05,
        help='æ£€æµ‹é˜ˆå€¼ (é»˜è®¤: 0.05)'
    )
    
    parser.add_argument(
        '--config', '-c',
        type=str,
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.json)'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='å¯ç”¨è¯¦ç»†è¾“å‡º'
    )
    
    parser.add_argument(
        '--output-dir', '-o',
        type=str,
        default='.',
        help='è¾“å‡ºç›®å½• (é»˜è®¤: å½“å‰ç›®å½•)'
    )
    
    return parser.parse_args()

def main(args=None):
    """ä¸»å‡½æ•°"""
    if args is None:
        args = parse_arguments()
    
    print("ğŸš€ OmniParser + GPT-4o æ¼”ç¤º")
    print("=" * 50)
    
    if args.verbose:
        print(f"ğŸ“‹ å‚æ•°é…ç½®:")
        print(f"   è®¾å¤‡: {args.device}")
        print(f"   é˜ˆå€¼: {args.threshold}")
        print(f"   è¾“å‡ºç›®å½•: {args.output_dir}")
        if args.image:
            print(f"   æŒ‡å®šå›¾åƒ: {args.image}")
    
    # 1. æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_path = args.config if args.config else os.path.join(project_root, "config.json")
    if not os.path.exists(config_path):
        print("âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼è¯·ç¡®ä¿ config.json å·²æ­£ç¡®è®¾ç½®")
        print(f"   æœŸæœ›è·¯å¾„: {config_path}")
        return
    
    try:
        config = get_config(config_path)
        print("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        if args.verbose:
            print(f"   ğŸ”§ APIç«¯ç‚¹: {config.get_openai_base_url()}")
            print(f"   ğŸ¤– ä½¿ç”¨æ¨¡å‹: {config.get_openai_model()}")
            print(f"   ğŸ“¦ æ‰¹å¤„ç†å¤§å°: {config.get_batch_size()}")
    except Exception as e:
        print(f"âŒ é…ç½®é”™è¯¯: {e}")
        return
    
    # 2. è®¾ç½®è®¾å¤‡
    if args.device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
        if device == 'cuda' and not torch.cuda.is_available():
            print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ° CPU")
            device = 'cpu'
    
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 3. æ£€æŸ¥è¾“å‡ºç›®å½•
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {args.output_dir}")
    
    # 4. åŠ è½½YOLOæ¨¡å‹
    model_path = os.path.join(project_root, 'weights/icon_detect/model.pt')
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("   è¯·ç¡®ä¿å·²ä¸‹è½½æ¨¡å‹æƒé‡æ–‡ä»¶")
        return
    
    print("\nğŸ“¥ åŠ è½½YOLOå›¾æ ‡æ£€æµ‹æ¨¡å‹...")
    som_model = get_yolo_model(model_path)
    som_model.to(device)
    print(f'âœ… æ¨¡å‹å·²åŠ è½½åˆ° {device}')
    
    # 4. é…ç½®GPT-4oå›¾æ ‡æè¿°æ¨¡å‹
    print("\nğŸ¤– é…ç½®GPT-4oå›¾æ ‡æè¿°æ¨¡å‹...")
    caption_model_processor = get_caption_model_processor(
        model_name="gpt4o", 
        model_name_or_path=config.get_openai_model(),
        device=device
    )
    print("âœ… GPT-4oæ¨¡å‹é…ç½®å®Œæˆ")
    
    # 6. ç¡®å®šè¦å¤„ç†çš„å›¾åƒ
    if args.image:
        test_images = args.image
    else:
        # é»˜è®¤å›¾åƒ
        test_images = [
            os.path.join(project_root, 'imgs/word.png'),
            os.path.join(project_root, 'imgs/windows_home.png'),
            os.path.join(project_root, 'imgs/google_page.png')
        ]
    
    print(f"ğŸ“¸ å°†å¤„ç† {len(test_images)} ä¸ªå›¾åƒ")
    
    for image_path in test_images:
        if not os.path.exists(image_path):
            print(f"âš ï¸  è·³è¿‡ä¸å­˜åœ¨çš„å›¾åƒ: {image_path}")
            continue
            
        print(f"\nğŸ–¼ï¸  å¤„ç†å›¾åƒ: {image_path}")
        print("-" * 40)
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path)
        image_rgb = image.convert('RGB')
        print(f'ğŸ“ å›¾åƒå°ºå¯¸: {image.size}')
        
        # é…ç½®è¾¹ç•Œæ¡†ç»˜åˆ¶å‚æ•°
        box_overlay_ratio = max(image.size) / 3200
        draw_bbox_config = {
            'text_scale': 0.8 * box_overlay_ratio,
            'text_thickness': max(int(2 * box_overlay_ratio), 1),
            'text_padding': max(int(3 * box_overlay_ratio), 1),
            'thickness': max(int(3 * box_overlay_ratio), 1),
        }
        BOX_TRESHOLD = args.threshold
        
        # è®¡æ—¶å¼€å§‹
        start_time = time.time()
        
        try:
            # OCR æ£€æµ‹
            print("ğŸ” è¿›è¡ŒOCRæ–‡æœ¬æ£€æµ‹...")
            ocr_start = time.time()
            ocr_bbox_rslt, is_goal_filtered = check_ocr_box(
                image_path, 
                display_img=False, 
                output_bb_format='xyxy', 
                goal_filtering=None, 
                easyocr_args={'paragraph': False, 'text_threshold': 0.9}, 
                use_paddleocr=True
            )
            text, ocr_bbox = ocr_bbox_rslt
            ocr_time = time.time() - ocr_start
            print(f"   ğŸ“ OCRå®Œæˆï¼Œæ£€æµ‹åˆ° {len(text)} ä¸ªæ–‡æœ¬åŒºåŸŸ (è€—æ—¶: {ocr_time:.2f}s)")
            
            # å›¾æ ‡æ£€æµ‹å’Œæè¿°
            print("ğŸ¯ è¿›è¡Œå›¾æ ‡æ£€æµ‹å’ŒGPT-4oæè¿°...")
            caption_start = time.time()
            
            dino_labled_img, label_coordinates, parsed_content_list = get_som_labeled_img(
                image_path, 
                som_model, 
                BOX_TRESHOLD=BOX_TRESHOLD, 
                output_coord_in_ratio=True, 
                ocr_bbox=ocr_bbox,
                draw_bbox_config=draw_bbox_config, 
                caption_model_processor=caption_model_processor, 
                ocr_text=text,
                use_local_semantics=True, 
                iou_threshold=0.7, 
                scale_img=False
                # batch_size ä¼šä»é…ç½®æ–‡ä»¶è‡ªåŠ¨è¯»å–
            )
            
            caption_time = time.time() - caption_start
            total_time = time.time() - start_time
            
            print(f"   âœ… å›¾æ ‡è¯†åˆ«å®Œæˆ (è€—æ—¶: {caption_time:.2f}s)")
            print(f"   ğŸ¯ æ€»å…±æ£€æµ‹åˆ° {len(parsed_content_list)} ä¸ªå…ƒç´ ")
            print(f"   â±ï¸  æ€»è€—æ—¶: {total_time:.2f}s")
            
            # ä¿å­˜æ ‡æ³¨å›¾åƒ
            output_path = os.path.join(args.output_dir, f"output_gpt4o_{os.path.basename(image_path)}")
            image_data = base64.b64decode(dino_labled_img)
            output_image = Image.open(io.BytesIO(image_data))
            output_image.save(output_path)
            print(f"   ğŸ’¾ æ ‡æ³¨å›¾åƒå·²ä¿å­˜: {output_path}")
            
            # æ˜¾ç¤ºè§£æç»“æœ
            print(f"\nğŸ“‹ è§£æç»“æœè¯¦æƒ…:")
            print("=" * 60)
            
            # åˆ›å»ºDataFrameæ˜¾ç¤ºç»“æœï¼ˆç±»ä¼¼notebookä¸­çš„æ˜¾ç¤ºï¼‰
            df = pd.DataFrame(parsed_content_list)
            df['ID'] = range(len(df))
            
            # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤º
            text_items = [item for item in parsed_content_list if item.get('type') == 'text']
            icon_items = [item for item in parsed_content_list if item.get('type') == 'icon']
            
            print(f"ğŸ“ æ–‡æœ¬å…ƒç´  ({len(text_items)} ä¸ª):")
            for i, item in enumerate(text_items):
                content = item.get('content', 'N/A')
                bbox = item.get('bbox', [])
                print(f"   {i+1:2d}. {content}")
                if bbox:
                    print(f"       ä½ç½®: ({bbox[0]:.3f}, {bbox[1]:.3f}, {bbox[2]:.3f}, {bbox[3]:.3f})")
            
            print(f"\nğŸ¯ å›¾æ ‡å…ƒç´  ({len(icon_items)} ä¸ª) - GPT-4oæè¿°:")
            for i, item in enumerate(icon_items):
                content = item.get('content', 'N/A')
                bbox = item.get('bbox', [])
                print(f"   {i+1:2d}. {content}")
                if bbox:
                    print(f"       ä½ç½®: ({bbox[0]:.3f}, {bbox[1]:.3f}, {bbox[2]:.3f}, {bbox[3]:.3f})")
            
            # ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSV
            csv_path = os.path.join(args.output_dir, f"results_gpt4o_{os.path.basename(image_path).replace('.png', '.csv')}")
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {csv_path}")
            
        except Exception as e:
            print(f"âŒ å¤„ç†å›¾åƒæ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
        
        print("\n" + "="*60)

if __name__ == "__main__":
    args = parse_arguments()
    try:
        main(args)
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"  - {args.output_dir}/output_gpt4o_*.png: æ ‡æ³¨å›¾åƒ")
        print(f"  - {args.output_dir}/results_gpt4o_*.csv: è¯¦ç»†ç»“æœæ•°æ®")
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­äº†å¤„ç†è¿‡ç¨‹")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc() 