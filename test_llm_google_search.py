#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
LLM Googleæœç´¢é¡µé¢æ“ä½œæµ‹è¯•è„šæœ¬
åŸºäºOmniParserè§£æçš„é¡µé¢å…ƒç´ æµ‹è¯•LLMçš„æŒ‰é’®é€‰æ‹©èƒ½åŠ›
"""

import os
import json
import csv
from typing import Dict, List, Any

class GoogleSearchTestSuite:
    """Googleæœç´¢é¡µé¢LLMæ“ä½œæµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, config_path: str = "config.json"):
        """åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶"""
        self.config_path = config_path
        self.results = []
        
    def load_page_elements(self, csv_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½é¡µé¢å…ƒç´ æ•°æ®"""
        try:
            elements = []
            with open(csv_path, 'r', encoding='utf-8-sig') as f:  # ä½¿ç”¨utf-8-sigå¤„ç†BOM
                reader = csv.DictReader(f)
                for row in reader:
                    elements.append(row)
            print(f"âœ… æˆåŠŸåŠ è½½ {len(elements)} ä¸ªé¡µé¢å…ƒç´ ")
            return elements
        except Exception as e:
            print(f"âŒ åŠ è½½CSVæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def create_screen_context(self, elements: List[Dict[str, Any]]) -> str:
        """åˆ›å»ºå±å¹•ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context = "å½“å‰å±å¹•å…ƒç´ ä¿¡æ¯:\n"
        
        # åˆ†ç±»æ˜¾ç¤ºå…ƒç´ 
        text_elements = [e for e in elements if e['type'] == 'text']
        icon_elements = [e for e in elements if e['type'] == 'icon']
        
        context += f"\nğŸ“ æ–‡æœ¬å…ƒç´  ({len(text_elements)} ä¸ª):\n"
        for elem in text_elements:
            bbox_str = elem['bbox'].strip('[]').replace(' ', '')
            context += f"ID {elem['ID']}: '{elem['content']}' (ä½ç½®: {bbox_str})\n"
        
        context += f"\nğŸ¯ å›¾æ ‡å…ƒç´  ({len(icon_elements)} ä¸ª):\n"
        for elem in icon_elements:
            bbox_str = elem['bbox'].strip('[]').replace(' ', '')
            interactable = "âœ“" if elem['interactivity'] == 'True' else "âœ—"
            context += f"ID {elem['ID']}: {elem['content']} [{interactable}å¯äº¤äº’] (ä½ç½®: {bbox_str})\n"
        
        return context
    
    def generate_test_tasks(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæµ‹è¯•ä»»åŠ¡"""
        return [
            {
                "id": 1,
                "task": "åœ¨Googleæœç´¢æ¡†ä¸­æœç´¢'Pythonç¼–ç¨‹'",
                "description": "ç”¨æˆ·æƒ³è¦æœç´¢Pythonç¼–ç¨‹ç›¸å…³å†…å®¹",
                "expected_elements": ["Google Search", "æœç´¢åŠŸèƒ½"],
                "keywords": ["search", "æœç´¢", "google search"]
            },
            {
                "id": 2,
                "task": "ç‚¹å‡»'I'm Feeling Lucky'æŒ‰é’®",
                "description": "ç”¨æˆ·æƒ³è¦ä½¿ç”¨Googleçš„'æ‰‹æ°”ä¸é”™'åŠŸèƒ½",
                "expected_elements": ["I'm Feeling Luckye"],
                "keywords": ["feeling lucky", "æ‰‹æ°”ä¸é”™"]
            },
            {
                "id": 3,
                "task": "ç™»å½•Googleè´¦æˆ·",
                "description": "ç”¨æˆ·æƒ³è¦ç™»å½•åˆ°Googleè´¦æˆ·",
                "expected_elements": ["Sign in"],
                "keywords": ["sign in", "ç™»å½•", "login"]
            },
            {
                "id": 4,
                "task": "æ‰“å¼€Gmailé‚®ç®±",
                "description": "ç”¨æˆ·æƒ³è¦è®¿é—®Gmailé‚®ç®±",
                "expected_elements": ["Gmail", "æ‰“å¼€Gmailé‚®ç®±"],
                "keywords": ["gmail", "é‚®ç®±", "email"]
            },
            {
                "id": 5,
                "task": "ä½¿ç”¨è¯­éŸ³æœç´¢åŠŸèƒ½",
                "description": "ç”¨æˆ·æƒ³è¦é€šè¿‡è¯­éŸ³è¿›è¡Œæœç´¢",
                "expected_elements": ["è¯­éŸ³æœç´¢"],
                "keywords": ["è¯­éŸ³", "voice", "éº¦å…‹é£"]
            }
        ]
    
    def create_test_prompt(self, task: Dict[str, Any], screen_context: str) -> str:
        """åˆ›å»ºæµ‹è¯•æç¤ºè¯"""
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªç½‘é¡µè‡ªåŠ¨åŒ–åŠ©æ‰‹ï¼Œéœ€è¦åœ¨Googleæœç´¢é¡µé¢æ‰§è¡Œç”¨æˆ·æŒ‡å®šçš„ä»»åŠ¡ã€‚

ä»»åŠ¡: {task['task']}
æè¿°: {task['description']}

{screen_context}

è¯·åˆ†æå½“å‰é¡µé¢ï¼Œå¹¶å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

1. ä¸ºäº†å®Œæˆä»»åŠ¡"{task['task']}"ï¼Œä½ éœ€è¦ä¸å“ªä¸ªé¡µé¢å…ƒç´ äº¤äº’ï¼Ÿ
2. è¯·æä¾›è¯¥å…ƒç´ çš„IDå·
3. ç®€è¦è¯´æ˜ä½ çš„é€‰æ‹©ç†ç”±

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼š
```json
{{
    "selected_element_id": å…ƒç´ IDå·(æ•°å­—),
    "element_content": "é€‰ä¸­å…ƒç´ çš„æè¿°",
    "reasoning": "é€‰æ‹©ç†ç”±",
    "confidence": è¯„åˆ†(1-10),
    "alternative_elements": [å…¶ä»–å¯èƒ½çš„å…ƒç´ ID]
}}
```
"""
        return prompt
    
    def evaluate_response(self, task: Dict[str, Any], selected_id: int, elements: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¯„ä¼°å“åº”çš„æ­£ç¡®æ€§"""
        evaluation = {
            "task_id": task["id"],
            "task_name": task["task"],
            "success": False,
            "score": 0,
            "details": {}
        }
        
        # æŸ¥æ‰¾é€‰ä¸­çš„å…ƒç´ 
        selected_element = None
        for elem in elements:
            if int(elem['ID']) == selected_id:
                selected_element = elem
                break
        
        if not selected_element:
            evaluation["details"]["error"] = f"å…ƒç´ ID {selected_id} ä¸å­˜åœ¨"
            return evaluation
        
        element_content = selected_element['content']
        is_interactable = selected_element['interactivity'] == 'True'
        
        evaluation["details"]["selected_element"] = {
            "id": selected_id,
            "content": element_content,
            "interactable": is_interactable,
            "type": selected_element['type']
        }
        
        # æ£€æŸ¥æ˜¯å¦å¯äº¤äº’
        if not is_interactable:
            evaluation["details"]["warning"] = "é€‰ä¸­çš„å…ƒç´ ä¸å¯äº¤äº’"
            evaluation["score"] = 2
        else:
            evaluation["score"] = 5  # åŸºç¡€åˆ†
        
        # æ£€æŸ¥æ˜¯å¦ç¬¦åˆé¢„æœŸ
        content_lower = element_content.lower()
        keywords = task["keywords"]
        
        # æ£€æŸ¥å…³é”®è¯åŒ¹é…
        keyword_match = any(keyword.lower() in content_lower for keyword in keywords)
        
        if keyword_match:
            evaluation["success"] = True
            evaluation["score"] = min(10, evaluation["score"] + 5)
            evaluation["details"]["match_reason"] = "å…³é”®è¯åŒ¹é…"
        else:
            # æ£€æŸ¥é¢„æœŸå…ƒç´ 
            expected_elements = task["expected_elements"]
            element_match = any(expected.lower() in content_lower for expected in expected_elements)
            
            if element_match:
                evaluation["success"] = True
                evaluation["score"] = min(10, evaluation["score"] + 5)
                evaluation["details"]["match_reason"] = "é¢„æœŸå…ƒç´ åŒ¹é…"
        
        return evaluation
    
    def run_manual_test(self, elements: List[Dict[str, Any]]):
        """è¿è¡Œæ‰‹åŠ¨æµ‹è¯•ï¼ˆæ¨¡æ‹ŸLLMé€‰æ‹©ï¼‰"""
        print("\nğŸš€ å¼€å§‹LLM Googleæœç´¢æ“ä½œæµ‹è¯•")
        print("=" * 60)
        
        # åˆ›å»ºå±å¹•ä¸Šä¸‹æ–‡
        screen_context = self.create_screen_context(elements)
        
        # ç”Ÿæˆæµ‹è¯•ä»»åŠ¡
        tasks = self.generate_test_tasks()
        
        # æ‰§è¡Œæµ‹è¯•
        for task in tasks:
            print(f"\nğŸ¯ æµ‹è¯•ä»»åŠ¡ {task['id']}: {task['task']}")
            print("-" * 40)
            
            # æ˜¾ç¤ºæµ‹è¯•æç¤ºè¯
            prompt = self.create_test_prompt(task, screen_context)
            print("ğŸ“ ç”Ÿæˆçš„æµ‹è¯•æç¤ºè¯:")
            print(prompt[:300] + "..." if len(prompt) > 300 else prompt)
            
            # æ¨¡æ‹ŸLLMé€‰æ‹©ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰
            print("\nğŸ¤– æ¨¡æ‹ŸLLMé€‰æ‹©...")
            best_match_id = self.simulate_llm_choice(task, elements)
            
            if best_match_id is not None:
                # è¯„ä¼°é€‰æ‹©
                evaluation = self.evaluate_response(task, best_match_id, elements)
                self.results.append(evaluation)
                
                # æ˜¾ç¤ºç»“æœ
                if evaluation["success"]:
                    print(f"âœ… æµ‹è¯•é€šè¿‡! å¾—åˆ†: {evaluation['score']}/10")
                else:
                    print(f"âŒ æµ‹è¯•å¤±è´¥! å¾—åˆ†: {evaluation['score']}/10")
                
                elem = evaluation["details"]["selected_element"]
                print(f"   ğŸ“ é€‰ä¸­å…ƒç´ : ID {elem['id']} - {elem['content']}")
                print(f"   ğŸ”§ å¯äº¤äº’: {'æ˜¯' if elem['interactable'] else 'å¦'}")
                
                if "match_reason" in evaluation["details"]:
                    print(f"   âœ¨ åŒ¹é…åŸå› : {evaluation['details']['match_reason']}")
            else:
                print("âŒ æœªæ‰¾åˆ°åˆé€‚çš„å…ƒç´ ")
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_report()
    
    def simulate_llm_choice(self, task: Dict[str, Any], elements: List[Dict[str, Any]]) -> int | None:
        """æ¨¡æ‹ŸLLMçš„é€‰æ‹©é€»è¾‘"""
        keywords = task["keywords"]
        best_score = 0
        best_id = None
        
        # ä¼˜å…ˆè€ƒè™‘å¯äº¤äº’çš„å…ƒç´ 
        interactive_elements = [e for e in elements if e['interactivity'] == 'True']
        
        for elem in interactive_elements:
            content_lower = elem['content'].lower()
            score = 0
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            for keyword in keywords:
                if keyword.lower() in content_lower:
                    score += 10
            
            # æ£€æŸ¥é¢„æœŸå…ƒç´ 
            expected_elements = task["expected_elements"]
            for expected in expected_elements:
                if expected.lower() in content_lower:
                    score += 15
            
            if score > best_score:
                best_score = score
                best_id = int(elem['ID'])
        
        return best_id
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results if r["success"])
        average_score = sum(r["score"] for r in self.results) / total_tests if total_tests > 0 else 0
        
        print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"æˆåŠŸæµ‹è¯•: {successful_tests}")
        print(f"æˆåŠŸç‡: {successful_tests/total_tests*100:.1f}%")
        print(f"å¹³å‡å¾—åˆ†: {average_score:.1f}/10")
        
        print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for result in self.results:
            status = "âœ…" if result["success"] else "âŒ"
            print(f"{status} ä»»åŠ¡{result['task_id']}: {result['task_name']} - {result['score']}/10")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_data = {
            "summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "success_rate": successful_tests/total_tests*100 if total_tests > 0 else 0,
                "average_score": average_score
            },
            "detailed_results": self.results
        }
        
        with open("llm_google_search_test_report.json", "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: llm_google_search_test_report.json")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” LLM Googleæœç´¢é¡µé¢æ“ä½œèƒ½åŠ›æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥CSVæ–‡ä»¶
    csv_file = "results_gpt4o_google_page.csv"
    if not os.path.exists(csv_file):
        print(f"âŒ CSVæ–‡ä»¶ {csv_file} ä¸å­˜åœ¨ï¼")
        print("è¯·å…ˆè¿è¡Œ demo_gpt4o.py ç”Ÿæˆé¡µé¢è§£æç»“æœ")
        return
    
    try:
        # åˆ›å»ºæµ‹è¯•å¥—ä»¶
        test_suite = GoogleSearchTestSuite()
        
        # åŠ è½½é¡µé¢å…ƒç´ 
        elements = test_suite.load_page_elements(csv_file)
        if not elements:
            return
        
        # æ˜¾ç¤ºé¡µé¢å…ƒç´ æ¦‚è§ˆ
        print("\nğŸ“Š é¡µé¢å…ƒç´ ç»Ÿè®¡:")
        text_count = len([e for e in elements if e['type'] == 'text'])
        icon_count = len([e for e in elements if e['type'] == 'icon'])
        interactive_count = len([e for e in elements if e['interactivity'] == 'True'])
        
        print(f"   ğŸ“ æ–‡æœ¬å…ƒç´ : {text_count} ä¸ª")
        print(f"   ğŸ¯ å›¾æ ‡å…ƒç´ : {icon_count} ä¸ª")
        print(f"   ğŸ”§ å¯äº¤äº’å…ƒç´ : {interactive_count} ä¸ª")
        
        # è¿è¡Œæ¨¡æ‹Ÿæµ‹è¯•
        test_suite.run_manual_test(elements)
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 